---
title: "Computer lab 1 block 1"
author:
- Duc Tran
- William Wiik
- Sigme 
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
  html_document:
    df_print: paged
geometry: top=100pt,bottom=100pt,left=68pt,right=66pt
subtitle: 732A99
header-includes:
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{longtable}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{titling}
- \usepackage[swedish, english]{babel}
- \renewcommand{\headrulewidth}{0pt}
- \renewcommand{\and}{\\}
- \pretitle{\centering\vspace{0cm}{\large Laboration report in Computational Statistics
  \par}\vspace{4cm}\Huge\textbf}
- \posttitle{\vspace{1cm}\large\textbf{}\par}
- \preauthor{\centering\vspace{4cm}\normalsize}
- \postauthor{\par\vspace{4cm}}
- \predate{\centering{\normalsize Division of Statistics and Machine Learning \\ Department
  of Computer Science \\ Linköping University \par}}
- \postdate{\par\vspace{2cm}}
- \raggedbottom
---

<!-- <!-- Väljer språk till svenska för automatiska titlar -->
<!-- \selectlanguage{swedish} -->

<!-- Byter språket på figur- och tabellbeskrivningar till angivna namn -->
\captionsetup[table]{name = Table}


<!-- Anger sidnumreringens position -->
\fancyhf{}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

<!-- Tar bort sidnumrering för förteckningar och titelsidan -->
\pagenumbering{gobble}

<!-- Anger sidbrytning -->
\clearpage

<!-- Skapar en innehållsförteckning och anger djupet av rubrikerna som ska visas -->
\setcounter{tocdepth}{3}
\tableofcontents

<!-- Anger sidbrytning -->
\clearpage

<!-- Börjar sidnumreringen på sida 1 efter att alla förteckningar visats -->
\pagenumbering{arabic}
\setcounter{page}{1}

<!-- Börjar med kapitel 1 -->

```{r options, echo = FALSE}
library(ggplot2)
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 4.5, 
  fig.height = 3)
```



# Assignment 3. Logistic regression and basis function expansion

## Data

The data contains information about the onset of
diabetes within 5 years in Pima Indians given medical details.
The variables are:

* Number of times pregnant

* Plasma glucose concentration a 2 hours in an oral glucose tolerance test.

* Diastolic blood pressure (mm Hg).

* Triceps skinfold thickness (mm).

* 2-Hour serum insulin (mu U/ml).

* Body mass index (weight in kg/(height in m)^2).

* Diabetes pedigree function.

* Age (years).

* Diabetes (0=no or 1=yes).


```{r}

# Reading data
diabetes_df <- read.csv("pima-indians-diabetes.csv", header=FALSE)

colnames(diabetes_df) <- c("times_pregnant", "plasma_glucose_conc",
                        "diastolic_blood_pressure", "triceps_skinfold_thickness",
                        "serum_insulin","body_mass_index", "diabetes_pedigree",
                        "age", "diabetes")

diabetes_df$diabetes <- ifelse(diabetes_df$diabetes == 0, "no", "yes")
diabetes_df$diabetes <- as.factor(diabetes_df$diabetes)


```


## 3.1


**Question:** 
Make a scatterplot showing a Plasma glucose concentration on Age where
observations are colored by Diabetes levels.


```{r}

library(ggplot2)

ggplot(diabetes_df, aes(x = plasma_glucose_conc, y = age, color = diabetes)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Diabetes",
       x = "Plasma glucose concentration",
       y = "Age")

```


**Question:**
Do you think that Diabetes is easy to classify by a standard logistic regression model that uses these two variables as
features? Motivate your answer.


**Answer**
We believe that age and plasma glucose concentration are not suitable variables for classifying diabetes because there's no clear relationship with the disease.


## 3.2

**Question:** 

Train a logistic regression model with $y$ = Diabetes as target $x_1$ = Plasma glucose
concentration and $x_2$ = Age as features and make a prediction for all observations
by using $r$ = 0.5 as the classification threshold. Report the probabilistic equation of the estimated model
and compute also the training misclassification error.

The probabilistic equation:

$$p(y = 1) = \frac{1}{1 + e^{-(\theta_0 + \theta_1 \cdot \text{Plasma glucose} + \theta_2 \cdot \text{Age})}}$$

$$p(y = 1) = \frac{1}{1 + e^{-(-5.91 + 0.04 \cdot \text{Plasma glucose} + 0.02 \cdot \text{Age})}}$$
$$\hat{y} = 1 \text{ if } p(y = 1) > 0.5$$


```{r}

model <- glm(diabetes ~ plasma_glucose_conc + age, data = diabetes_df,
             family =  "binomial")

pred <- predict(model, newdata = diabetes_df, type = "response")

# Using 0.5 as the classification threshold
pred <- ifelse(pred > 0.5, "yes", "no")

# cunfusion matrix to calculate the misclassification error
confusion <- table(diabetes_df$diabetes, pred)
misclass_rate <- (confusion[1,2] + confusion[2,1]) / sum(confusion)




```

```{r echo=FALSE}


knitr::kable(as.data.frame(round(misclass_rate,2)), col.names = "Misclassification error",
             caption = "Misclassification error")

```

The misclassification error is 0.26.



**Question:**

Make a scatter plot of the same kind as in step 1 but showing the predicted values of Diabetes as a color instead.

```{r}

diabetes_df_pred <- diabetes_df
diabetes_df_pred$pred <- pred
  
ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```

**Question:**

Comment on the quality of the classification by using these results.


**Answer**

The quality of the classification is not best, because approximately 26% of the observations in our training data are incorrectly classified by the logistic regression model.
If we compare this figure X to the figure Y in step 3.1 we can see that the predicted values from figure X do not entirely align with the true values from figure Y.



## 3.3

**Question:**

Use the model estimated in step 2 to a) report the equation of the decision boundary between the two classes b) add a curve showing this boundary to the scatter plot in step 2.


The decision boundary equation:

The decision boundary occurs when the predicted probability is equal to the threshold $r$.

$$\frac{1}{1 + e^{-(\theta_0 + \theta_1 \cdot \text{Plasma glucose} + \theta_2 \cdot \text{Age})}} =0.5 \Rightarrow \theta_0 + \theta_1 \cdot\text{Plasma glucose} + \theta_2 \cdot \text{Age} = 0$$
$$ -5.91 + 0.04 \cdot \text{Plasma glucose} + 0.02 \cdot \text{Age} = 0$$

```{r}


ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
  geom_point() +
  theme_bw() +
 stat_function(fun = ({function(x) (-coef(model)[1] - coef(model)[2]*x)/ coef(model)[3] }),
               size=1.5, color = "black") +
  ylim(20,90) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```


**Question:**

Comment whether the decision boundary seems to catch the data distribution well.

**Answer**

The decision boundary appears to capture the data distribution appropriately. For instance, the cluster with Plasma glucose concentration between 75 to 150 and age 20 to 30 is correctly predicted as class "no" by the decision boundary, aligning with the actual class (figure Y). 
However, it is notable that the boundary between classes does not show a linear pattern in figure Y, which means that
a linear decision boundary is never going to catch the data distribution very well.



## 3.4

**Question:**

Make same kind of plots as in step 2 but use thresholds $r$ = 0.2 and $r$ = 0.8. By
using these plots.


```{r}
library("ggpubr")

# Using 0.2 as the classification threshold
pred <- predict(model, newdata = diabetes_df, type = "response")
pred <- ifelse(pred > 0.2, "yes", "no")
diabetes_df_pred$pred <- pred


p1 <- ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
      geom_point() +
      theme_bw() +
      labs(colour = "Predicted values of diabetes",
           x = "Plasma glucose concentration",
           y = "Age") +
      ggtitle("r = 0.2")


# Using 0.8 as the classification threshold
pred <- predict(model, newdata = diabetes_df, type = "response")
pred <- ifelse(pred > 0.8, "yes", "no")
diabetes_df_pred$pred <- pred


p2 <- ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
      geom_point() +
      theme_bw() +
      labs(colour = "Predicted values of diabetes",
           x = "Plasma glucose concentration",
           y = "Age") +
  ggtitle("r = 0.8")


ggarrange(p1, p2,  ncol = 1, nrow = 2)

```

**Question:**

Comment on what happens with the prediction when $r$ value changes.

**Answer**


As $r$ increases, the model predict more observations as "no" for diabetes.
Opposite, as $r$ decreases, the model predict more observations as "yes" for diabetes.

## 3.5

**Question:**

Perform a basis function expansion trick by computing new features $z_1 = x_1^4$,
$z_2 = x_1^3x^2$, $z_3 = x_1^2x_2^2$, $z_4 = x_1x_2^3$, $z_5 = x_2^4$, adding them to the data set and
then computing a logistic regression model with $y$ as target and $x_1,x_2,z_1,...,z_5$ as features.
Create a scatterplot of the same kind as in step 2 for this model and compute the training misclassification rate.


```{r}

# new features
diabetes_df$z1  <- diabetes_df$times_pregnant^4
diabetes_df$z2  <- diabetes_df$times_pregnant^3 * diabetes_df$plasma_glucose_conc^2
diabetes_df$z3  <- diabetes_df$times_pregnant^2 * diabetes_df$plasma_glucose_conc^2
diabetes_df$z4  <- diabetes_df$times_pregnant * diabetes_df$plasma_glucose_conc^3
diabetes_df$z5  <- diabetes_df$plasma_glucose_conc^4


model <- glm(diabetes ~ plasma_glucose_conc + age + z1 + z2 + z3 + z4 + z5, data = diabetes_df,
             family =  "binomial")

pred <- predict(model, newdata = diabetes_df, type = "response")

# Using 0.5 as the classification threshold
pred <- ifelse(pred > 0.5, "yes", "no")

# cunfusion matrix to calculate the misclassification error
confusion <- table(diabetes_df$diabetes, pred)
misclass_rate <- (confusion[1,2] + confusion[2,1]) / sum(confusion)


diabetes_df_pred <- diabetes_df
diabetes_df_pred$pred <- pred
  
ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```


```{r echo=FALSE}


knitr::kable(as.data.frame(round(misclass_rate,2)), col.names = "Misclassification error",
             caption = "Misclassification error")

```


The misclassification error is 0.25.

**Question:**

What can you say about the quality of this model compared to the previous logistic
regression model? How have the basis expansion trick affected the shape of the decision boundary and the prediction accuracy?


**Answer** 

The misclassification rate is one percentage point lower than the model in 3.2, indicating a slight improvement.

The decision boundary's shape is not linear, as seen in 3.4, but the data distribution of the predicted values remains very similar.









