---
title: "Computer lab 1 block 1"
author:
- Duc Tran
- William Wiik
- Sigme 
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    number_sections: yes
geometry: top=100pt,bottom=100pt,left=68pt,right=66pt
subtitle: 732A99
header-includes:
- \usepackage{booktabs}
- \usepackage{float}
- \usepackage{longtable}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{titling}
- \usepackage[swedish, english]{babel}
- \renewcommand{\headrulewidth}{0pt}
- \renewcommand{\and}{\\}
- \pretitle{\centering\vspace{0cm}{\large Laboration report in Computational Statistics
  \par}\vspace{4cm}\Huge\textbf}
- \posttitle{\vspace{1cm}\large\textbf{}\par}
- \preauthor{\centering\vspace{4cm}\normalsize}
- \postauthor{\par\vspace{4cm}}
- \predate{\centering{\normalsize Division of Statistics and Machine Learning \\ Department
  of Computer Science \\ Linköping University \par}}
- \postdate{\par\vspace{2cm}}
- \raggedbottom
---

<!-- <!-- Väljer språk till svenska för automatiska titlar -->
<!-- \selectlanguage{swedish} -->

<!-- Byter språket på figur- och tabellbeskrivningar till angivna namn -->
\captionsetup[table]{name = Table}


<!-- Anger sidnumreringens position -->
\fancyhf{}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

<!-- Tar bort sidnumrering för förteckningar och titelsidan -->
\pagenumbering{gobble}

<!-- Anger sidbrytning -->
\clearpage

<!-- Skapar en innehållsförteckning och anger djupet av rubrikerna som ska visas -->
\setcounter{tocdepth}{3}
\tableofcontents

<!-- Anger sidbrytning -->
\clearpage

<!-- Börjar sidnumreringen på sida 1 efter att alla förteckningar visats -->
\pagenumbering{arabic}
\setcounter{page}{1}

<!-- Börjar med kapitel 1 -->

```{r options, echo = FALSE}
library(ggplot2)
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 4.5, 
  fig.height = 3)
```



# Assignment 3. Logistic regression and basis function expansion

## Data

The data contains information about the onset of
diabetes within 5 years in Pima Indians given medical details.
The variables are:

* Number of times pregnant

* Plasma glucose concentration a 2 hours in an oral glucose tolerance test.

* Diastolic blood pressure (mm Hg).

* Triceps skinfold thickness (mm).

* 2-Hour serum insulin (mu U/ml).

* Body mass index (weight in kg/(height in m)^2).

* Diabetes pedigree function.

* Age (years).

* Diabetes (0=no or 1=yes).


```{r}

diabetes_df <- read.csv("pima-indians-diabetes.csv", header=FALSE)

colnames(diabetes_df) <- c("times_pregnant", "plasma_glucose_conc",
                        "diastolic_blood_pressure", "triceps_skinfold_thickness",
                        "serum_insulin","body_mass_index", "diabetes_pedigree",
                        "age", "diabetes")

diabetes_df$diabetes <- ifelse(diabetes_df$diabetes == 0, "no", "yes")
diabetes_df$diabetes <- as.factor(diabetes_df$diabetes)


```


## 3.1


**Question:** 
Make a scatterplot showing a Plasma glucose concentration on Age where
observations are colored by Diabetes levels.


```{r}

library(ggplot2)

ggplot(diabetes_df, aes(x = plasma_glucose_conc, y = age, color = diabetes)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Diabetes",
       x = "Plasma glucose concentration",
       y = "Age")

```


**Question:**
Do you think that Diabetes is easy to classify by a standard logistic regression model that uses these two variables as
features? Motivate your answer.

We don't think these two variables are good variables to classify Diabetes because
there is no clear relationship between age, plasma glucose concentration with Diabetes.


## 3.2

**Question:** 

Train a logistic regression model with $y$ = Diabetes as target $x_1$ = Plasma glucose
concentration and $x_2$ = Age as features and make a prediction for all observations
by using $r$ = 0.5 as the classification threshold. Report the probabilistic equation of the estimated model
and compute also the training misclassification error.

The probabilistic equation:

$$$$


```{r}

model <- glm(diabetes ~ plasma_glucose_conc + age, data = diabetes_df,
             family =  "binomial")

pred <- predict(model, newdata = diabetes_df, type = "response")

# Using 0.5 as the classification threshold
pred <- ifelse(pred > 0.5, "yes", "no")

confusion <- table(diabetes_df$diabetes, pred)
misclass_rate <- (confusion[1,2] + confusion[2,1]) / sum(confusion)


knitr::kable(as.data.frame(round(misclass_rate,2)), col.names = "Misclassification error",
             caption = "Misclassification error")

```



**Question:**

Make a scatter plot of the same kind as in step 1 but showing the predicted values of Diabetes as a color instead.

```{r}

diabetes_df_pred <- diabetes_df
diabetes_df_pred$pred <- pred
  
ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```

**Question:**

Comment on the quality of the classification by using these results.


## 3.3

**Question:**

Use the model estimated in step 2 to a) report the equation of the decision boundary between the two classes b) add a curve showing this boundary to the scatter plot in step 2.


The decision boundary equation:

$$\beta_0 + \beta_1 \cdot \text{plasma glucos econc}$$

```{r}

summary(model)

slope <- coef(model)[2]/(-coef(model)[3])
intercept <- coef(model)[1]/(-coef(model)[3]) 


diabetes_df


 #coef(model)[1] + t(as.matrix(coef(model)[2:3])) %*% as.matrix(diabetes_df[,c("plasma_glucose_conc","age")])

ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
  geom_point() +
  theme_bw() +
 stat_function(fun = ({function(x) (-coef(model)[1] - coef(model)[2]*x)/ coef(model)[3] }),
               size=1.5, color = "black") +
  ylim(20,90) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```


**Question:**

Comment whether the decision boundary seems to catch the data distribution well.


## 3.4

**Question:**

Make same kind of plots as in step 2 but use thresholds $r$ = 0.2 and $r$ = 0.8. By
using these plots


```{r}
library("ggpubr")

# Using 0.2 as the classification threshold
pred <- predict(model, newdata = diabetes_df, type = "response")
pred <- ifelse(pred > 0.2, "yes", "no")
diabetes_df_pred$pred <- pred


p1 <- ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
      geom_point() +
      theme_bw() +
      labs(colour = "Predicted values of diabetes",
           x = "Plasma glucose concentration",
           y = "Age") +
      ggtitle("p = 0.2")


# Using 0.8 as the classification threshold
pred <- predict(model, newdata = diabetes_df, type = "response")
pred <- ifelse(pred > 0.8, "yes", "no")
diabetes_df_pred$pred <- pred


p2 <- ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) +
      geom_point() +
      theme_bw() +
      labs(colour = "Predicted values of diabetes",
           x = "Plasma glucose concentration",
           y = "Age") +
  ggtitle("p = 0.8")


ggarrange(p1, p2,  ncol = 1, nrow = 2)

```

**Question:**

Comment on what happens with the prediction when $r$ value changes.

## 3.5

**Question:**

Perform a basis function expansion trick by computing new features $z_1 = x_1^4$,
$z_2 = x_1^3x^2$, $z_3 = x_1^2x_2^2$, $z_4 = x_1x_2^3$, $z_5 = x_2^4$, adding them to the data set and
then computing a logistic regression model with $y$ as target and $x_1,x_2,z_1,...,z_5$ as features.
Create a scatterplot of the same kind as in step 2 for this model.


```{r}

diabetes_df$z1  <- diabetes_df$times_pregnant^4
diabetes_df$z2  <- diabetes_df$times_pregnant^3 * diabetes_df$plasma_glucose_conc^2
diabetes_df$z3  <- diabetes_df$times_pregnant^2 * diabetes_df$plasma_glucose_conc^2
diabetes_df$z4  <- diabetes_df$times_pregnant * diabetes_df$plasma_glucose_conc^3
diabetes_df$z5  <- diabetes_df$plasma_glucose_conc^4


model <- glm(diabetes ~ plasma_glucose_conc + age + z1 + z2 + z3 + z4 + z5, data = diabetes_df,
             family =  "binomial")

pred <- predict(model, newdata = diabetes_df, type = "response")

# Using 0.5 as the classification threshold
pred <- ifelse(pred > 0.5, "yes", "no")

diabetes_df_pred <- diabetes_df
diabetes_df_pred$pred <- pred
  
ggplot(diabetes_df_pred, aes(x = plasma_glucose_conc, y = age, color = pred)) + 
  geom_point() + 
  theme_bw() + 
  theme(axis.title.y = element_text(angle = 0,vjust = 0.5)) +
  labs(colour = "Predicted values of diabetes",
       x = "Plasma glucose concentration",
       y = "Age")


```


**Question:**

Compute the training misclassification rate. What can you say about the quality of this model compared to the previous logistic
regression model? How have the basis expansion trick affected the shape of the decision boundary and the prediction accuracy?















